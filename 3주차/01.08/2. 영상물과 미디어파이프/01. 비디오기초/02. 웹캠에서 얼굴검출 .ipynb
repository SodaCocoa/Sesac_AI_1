{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-block alert-info\"> [Part1] 웹캠에서 비디오 인식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "webcam = cv2.VideoCapture(0)\n",
    "webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "while webcam.isOpened():\n",
    "    ret, frame = webcam.read()\n",
    "    if ret:\n",
    "       cv2.imshow(\"test\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-block alert-info\"> [Part2] 웹캠에서 비디오에서 정면 얼굴파악\n",
    "- 약 1분 동안 카메라를 바라보고 정면 인식율이 얼마나 되는지를 확인하는 프로그램을 제작하여 보세요\n",
    "- 출력물 (여기서 시간은 시스템 시간이 표시되게 함.)\n",
    " - 시간(시:분:초), 모습\n",
    "   - 9:20:20초         0\n",
    "   - 9:20:21초         X\n",
    "   - 9:20:22초         X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:26     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:27     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n",
      "17:05:28     0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        # Load the pre-trained face cascade classifier\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "        # Open the webcam\n",
    "        self.webcam = cv2.VideoCapture(0)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            print(\"Could not open webcam\")\n",
    "            exit()\n",
    "\n",
    "    def detect_faces(self):\n",
    "        ret, frame = self.webcam.read()\n",
    "        if ret:\n",
    "            # Convert the frame to grayscale for face detection\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Perform face detection\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            # Get the current system time\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                # Faces detected\n",
    "                print(f\"{current_time}     X\")\n",
    "            else:\n",
    "                # No faces detected\n",
    "                print(f\"{current_time}     0\")\n",
    "\n",
    "            # Display the frame with rectangles around detected faces\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.webcam.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            exit()\n",
    "\n",
    "def main():\n",
    "    face_detector = FaceDetector()\n",
    "\n",
    "    while face_detector.webcam.isOpened():\n",
    "        face_detector.detect_faces()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl.metadata (2.7 kB)\n",
      "Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.1 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 41.0/164.1 kB 991.0 kB/s eta 0:00:01\n",
      "   -------------- ------------------------- 61.4/164.1 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------- --------- 122.9/164.1 kB 901.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 164.1/164.1 kB 986.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#화면 캡쳐만 코드\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0) # 노트북 웹캠을 카메라로 사용\n",
    "cap.set(3,640) # 너비\n",
    "cap.set(4,480) # 높이\n",
    "\n",
    "ret, frame = cap.read() # 사진 촬영\n",
    "frame = cv2.flip(frame, 1) # 좌우 대칭\n",
    "\n",
    "cv2.imwrite('self camera test.jpg', frame) # 사진 저장\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:06:28     0\n",
      "17:06:28     0\n",
      "17:06:28     0\n",
      "17:06:28     0\n",
      "17:06:28     0\n",
      "17:06:28     0\n",
      "17:06:28     0\n",
      "17:06:28     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:29     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:30     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:31     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:32     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:33     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:34     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:35     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     X\n",
      "17:06:36     0\n",
      "17:06:36     0\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     0\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:37     X\n",
      "17:06:38     X\n",
      "17:06:38     X\n",
      "17:06:38     X\n",
      "17:06:38     X\n",
      "17:06:38     X\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:38     0\n",
      "17:06:39     0\n",
      "17:06:39     0\n",
      "17:06:39     0\n",
      "17:06:39     0\n",
      "17:06:39     0\n",
      "17:06:39     X\n",
      "17:06:39     X\n",
      "17:06:39     X\n",
      "17:06:39     X\n",
      "17:06:39     X\n",
      "17:06:39     X\n",
      "17:06:39     X\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "\n",
    "class FaceMosaic:\n",
    "    def __init__(self, scale_factor=0.04):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def detect_faces(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "        return faces\n",
    "\n",
    "    def mosaic_faces(self, frame, faces):\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y + h, x:x + w]\n",
    "            face_img = cv2.resize(face_img, dsize=(0, 0), fx=self.scale_factor, fy=self.scale_factor)\n",
    "            face_img = cv2.resize(face_img, (w, h), interpolation=cv2.INTER_AREA)\n",
    "            frame[y:y + h, x:x + w] = face_img\n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    webcam = cv2.VideoCapture(0)\n",
    "    webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    face_mosaic = FaceMosaic()\n",
    "\n",
    "    if not webcam.isOpened():\n",
    "        print(\"웹캠을 열 수 없습니다\")\n",
    "        exit()\n",
    "\n",
    "    while webcam.isOpened():\n",
    "        ret, frame = webcam.read()\n",
    "        if ret:\n",
    "            faces = face_mosaic.detect_faces(frame)\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                print(f\"{current_time}     X\")\n",
    "                frame = face_mosaic.mosaic_faces(frame, faces)\n",
    "            else:\n",
    "                print(f\"{current_time}     0\")\n",
    "\n",
    "            cv2.imshow(\"얼굴 검출\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:15     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:16     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     X\n",
      "17:07:17     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:18     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     0\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:19     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n",
      "17:07:20     X\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "\n",
    "def capture_photo(frame):\n",
    "    # 캡처된 프레임을 파일로 저장합니다\n",
    "    captured_frame = cv2.flip(frame, 1)  # 올바른 방향으로 뒤집기\n",
    "    cv2.imwrite('self_camera_test.jpg', captured_frame)\n",
    "    print(\"사진을 캡처하고 'self_camera_test.jpg'로 저장되었습니다.\")\n",
    "\n",
    "def main():\n",
    "    # 웹캠을 엽니다\n",
    "    webcam = cv2.VideoCapture(0)\n",
    "    webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    face_mosaic = FaceMosaic()\n",
    "\n",
    "    if not webcam.isOpened():\n",
    "        print(\"웹캠을 열 수 없습니다\")\n",
    "        exit()\n",
    "\n",
    "    while webcam.isOpened():\n",
    "        ret, frame = webcam.read()\n",
    "        if ret:\n",
    "            faces = face_mosaic.detect_faces(frame)\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                print(f\"{current_time}     X\")\n",
    "                frame = face_mosaic.mosaic_faces(frame, faces)\n",
    "            else:\n",
    "                print(f\"{current_time}     0\")\n",
    "\n",
    "            cv2.imshow(\"얼굴 검출\", frame)\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('y'):\n",
    "                capture_photo(frame)\n",
    "                break  # 'y'를 누르면 캡처 후 종료\n",
    "\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    # 웹캠을 해제하고 모든 OpenCV 창을 닫습니다\n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:34     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     0\n",
      "14:50:35     X\n",
      "14:50:35     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     X\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:36     X\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:36     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:37     0\n",
      "14:50:38     0\n",
      "14:50:38     0\n",
      "14:50:38     0\n",
      "14:50:38     0\n",
      "14:50:38     0\n",
      "14:50:38     0\n",
      "14:50:38     0\n",
      "14:50:38     0\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:38     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "14:50:39     X\n",
      "사진을 캡처하고 'self_camera_test.jpg'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "class FaceMosaic:\n",
    "    def __init__(self, scale_factor=0.04):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.scale_factor = scale_factor\n",
    "        self.face_data = []  # 얼굴 데이터를 저장할 리스트\n",
    "\n",
    "    def detect_faces(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "        return faces\n",
    "\n",
    "    def mosaic_faces(self, frame, faces):\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y + h, x:x + w]\n",
    "            face_img = cv2.resize(face_img, dsize=(0, 0), fx=self.scale_factor, fy=self.scale_factor)\n",
    "            face_img = cv2.resize(face_img, (w, h), interpolation=cv2.INTER_AREA)\n",
    "            frame[y:y + h, x:x + w] = face_img\n",
    "        return frame\n",
    "\n",
    "    def record_face_data(self, faces_detected):\n",
    "        # 각 순간의 얼굴 여부와 시간을 저장\n",
    "        current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "        self.face_data.append({'time': current_time, 'face_detected': faces_detected})\n",
    "\n",
    "    def save_face_data_to_csv(self):\n",
    "        # CSV 파일에 얼굴 데이터를 저장\n",
    "        with open('face_data.csv', 'w', newline='') as csvfile:\n",
    "            fieldnames = ['time', 'face_detected']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for data in self.face_data:\n",
    "                writer.writerow(data)\n",
    "\n",
    "def capture_photo(frame):\n",
    "    captured_frame = cv2.flip(frame, 1)  # Flip for correct orientation\n",
    "    cv2.imwrite('사진캡쳐.jpg', captured_frame)\n",
    "    print(\"사진을 캡처하고 '사진캡쳐.jpg'로 저장되었습니다.\")\n",
    "\n",
    "def main():\n",
    "    webcam = cv2.VideoCapture(0)\n",
    "    webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    face_mosaic = FaceMosaic()\n",
    "\n",
    "    if not webcam.isOpened():\n",
    "        print(\"웹캠을 열 수 없습니다\")\n",
    "        exit()\n",
    "\n",
    "    while webcam.isOpened():\n",
    "        ret, frame = webcam.read()\n",
    "        if ret:\n",
    "            faces = face_mosaic.detect_faces(frame)\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                print(f\"{current_time}     X\")\n",
    "                frame = face_mosaic.mosaic_faces(frame, faces)\n",
    "                face_mosaic.record_face_data(True)\n",
    "            else:\n",
    "                print(f\"{current_time}     0\")\n",
    "                face_mosaic.record_face_data(False)\n",
    "\n",
    "            cv2.imshow(\"얼굴 검출\", frame)\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('y'):\n",
    "                capture_photo(frame)\n",
    "                break  # 'y'를 누르면 캡처 후 종료\n",
    "\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    # 웹캠 사용 종료 시 CSV에 얼굴 데이터 저장\n",
    "    face_mosaic.save_face_data_to_csv()\n",
    "\n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mdlib\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "#스노우 어플같은거 cpp설치해야함\n",
    "import cv2, dlib, sys\n",
    "import numpy as np\n",
    "\n",
    "scaler = 0.3\n",
    "\n",
    "# initialize face detector and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# load video\n",
    "cap = cv2.VideoCapture(0)  # Use the laptop webcam as the camera\n",
    "cap.set(3, 640)  # Width\n",
    "cap.set(4, 480)  # Height\n",
    "# load overlay image\n",
    "overlay = cv2.imread('dog.jpg', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# overlay function\n",
    "def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):\n",
    "  bg_img = background_img.copy()\n",
    "  # convert 3 channels to 4 channels\n",
    "  if bg_img.shape[2] == 3:\n",
    "    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "  if overlay_size is not None:\n",
    "    img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)\n",
    "\n",
    "  b, g, r, a = cv2.split(img_to_overlay_t)\n",
    "\n",
    "  mask = cv2.medianBlur(a, 5)\n",
    "\n",
    "  h, w, _ = img_to_overlay_t.shape\n",
    "  roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]\n",
    "\n",
    "  img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))\n",
    "  img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)\n",
    "\n",
    "  bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)\n",
    "\n",
    "  # convert 4 channels to 4 channels\n",
    "  bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "  return bg_img\n",
    "\n",
    "face_roi = []\n",
    "face_sizes = []\n",
    "\n",
    "# loop\n",
    "while True:\n",
    "  # read frame buffer from video\n",
    "  ret, img = cap.read()\n",
    "  if not ret:\n",
    "    break\n",
    "\n",
    "  # resize frame\n",
    "  img = cv2.resize(img, (int(img.shape[1] * scaler), int(img.shape[0] * scaler)))\n",
    "  ori = img.copy()\n",
    "\n",
    "  # find faces\n",
    "  if len(face_roi) == 0:\n",
    "    faces = detector(img, 1)\n",
    "  else:\n",
    "    roi_img = img[face_roi[0]:face_roi[1], face_roi[2]:face_roi[3]]\n",
    "    # cv2.imshow('roi', roi_img)\n",
    "    faces = detector(roi_img)\n",
    "\n",
    "  # no faces\n",
    "  if len(faces) == 0:\n",
    "    print('no faces!')\n",
    "\n",
    "  # find facial landmarks\n",
    "  for face in faces:\n",
    "    if len(face_roi) == 0:\n",
    "      dlib_shape = predictor(img, face)\n",
    "      shape_2d = np.array([[p.x, p.y] for p in dlib_shape.parts()])\n",
    "    else:\n",
    "      dlib_shape = predictor(roi_img, face)\n",
    "      shape_2d = np.array([[p.x + face_roi[2], p.y + face_roi[0]] for p in dlib_shape.parts()])\n",
    "\n",
    "    for s in shape_2d:\n",
    "      cv2.circle(img, center=tuple(s), radius=1, color=(255, 255, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # compute face center\n",
    "    center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int)\n",
    "\n",
    "    # compute face boundaries\n",
    "    min_coords = np.min(shape_2d, axis=0)\n",
    "    max_coords = np.max(shape_2d, axis=0)\n",
    "\n",
    "    # draw min, max coords\n",
    "    cv2.circle(img, center=tuple(min_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "    cv2.circle(img, center=tuple(max_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # compute face size\n",
    "    face_size = max(max_coords - min_coords)\n",
    "    face_sizes.append(face_size)\n",
    "    if len(face_sizes) > 10:\n",
    "      del face_sizes[0]\n",
    "    mean_face_size = int(np.mean(face_sizes) * 1.8)\n",
    "\n",
    "    # compute face roi\n",
    "    face_roi = np.array([int(min_coords[1] - face_size / 2), int(max_coords[1] + face_size / 2), int(min_coords[0] - face_size / 2), int(max_coords[0] + face_size / 2)])\n",
    "    face_roi = np.clip(face_roi, 0, 10000)\n",
    "\n",
    "    # draw overlay on face\n",
    "    result = overlay_transparent(ori, overlay, center_x + 8, center_y - 25, overlay_size=(mean_face_size, mean_face_size))\n",
    "\n",
    "  # visualize\n",
    "  cv2.imshow('original', ori)\n",
    "  cv2.imshow('facial landmarks', img)\n",
    "  cv2.imshow('result', result)\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 259 for command:\n",
      "        play voice.mp3 wait\n",
      "    지정한 명령 매개 변수를 드라이버가 인식할 수 없습니다.\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 259 for command:\n        play voice.mp3 wait\n    지정한 명령 매개 변수를 드라이버가 인식할 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     tts\u001b[38;5;241m.\u001b[39msave(filename)\n\u001b[0;32m      9\u001b[0m     playsound\u001b[38;5;241m.\u001b[39mplaysound(filename)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m감사합니다\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#웹캡에 얼굴을 넣고 얼굴을 인식하면 '사진을 촬영하겠습니까?' 음성 나오게 하고 yes 또는 네 음성을 인식해서 사진찍게해줘\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mspeak\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoice.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m tts\u001b[38;5;241m.\u001b[39msave(filename)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mplaysound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaysound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bluecom011\\miniconda3\\envs\\OpenAI\\Lib\\site-packages\\playsound.py:73\u001b[0m, in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     71\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m     winCommand(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sound))\n\u001b[1;32m---> 73\u001b[0m     \u001b[43mwinCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplay \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m wait\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bluecom011\\miniconda3\\envs\\OpenAI\\Lib\\site-packages\\playsound.py:64\u001b[0m, in \u001b[0;36m_playsoundWin.<locals>.winCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     60\u001b[0m     exceptionMessage \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Error \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(errorCode) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for command:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m command\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     62\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m errorBuffer\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     63\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(exceptionMessage)\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(exceptionMessage)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 259 for command:\n        play voice.mp3 wait\n    지정한 명령 매개 변수를 드라이버가 인식할 수 없습니다."
     ]
    }
   ],
   "source": [
    "#음성 출력\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    filename = 'voice.mp3'\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "speak(\"감사합니다\")\n",
    "#웹캡에 얼굴을 넣고 얼굴을 인식하면 '사진을 촬영하겠습니까?' 음성 나오게 하고 yes 또는 네 음성을 인식해서 사진찍게해줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import cv2\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.webcam = cv2.VideoCapture(0)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            print(\"Could not open webcam\")\n",
    "            exit()\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def detect_faces(self):\n",
    "        ret, frame = self.webcam.read()\n",
    "\n",
    "        if ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                speak(\"얼굴이 정면을 향하고 있습니다. 감사합니다.\")\n",
    "            else:\n",
    "                speak(\"얼굴을 감지할 수 없습니다. 얼굴을 정면으로 향하게 해주세요.\")\n",
    "\n",
    "            with self.lock:\n",
    "                for (x, y, w, h) in faces:\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                self.webcam.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    filename = os.path.join(script_dir, 'voice1.mp3')\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "def main():\n",
    "    face_detector = FaceDetector()\n",
    "\n",
    "    def process_frames():\n",
    "        while face_detector.webcam.isOpened():\n",
    "            face_detector.detect_faces()\n",
    "\n",
    "    # Start a separate thread for frame processing\n",
    "    frame_thread = threading.Thread(target=pqrocess_frames)\n",
    "    frame_thread.start()\n",
    "\n",
    "    frame_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import cv2\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.webcam = cv2.VideoCapture(0)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            print(\"Could not open webcam\")\n",
    "            exit()\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def detect_faces(self):\n",
    "        ret, frame = self.webcam.read()\n",
    "\n",
    "        if ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                speak(\"얼굴이 정면을 향하고 있습니다.\")\n",
    "                speak(\"얼굴 사진을 찍으시겠습니까?\")\n",
    "                response = input(\"얼굴 사진을 찍으시겠습니까? (y/n): \").lower()\n",
    "\n",
    "                if response == 'y':\n",
    "                    self.take_picture(frame)\n",
    "                elif response == 'n':\n",
    "                    speak(\"얼굴 사진을 찍지 않습니다. 프로그램을 종료합니다.\")\n",
    "                    self.webcam.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    exit()\n",
    "\n",
    "            else:\n",
    "                speak(\"얼굴을 감지할 수 없습니다. 얼굴을 정면으로 향하게 해주세요.\")\n",
    "\n",
    "            with self.lock:\n",
    "                for (x, y, w, h) in faces:\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                self.webcam.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "    def take_picture(self, frame):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y년%m월%d일_%H시%M분%S초\")\n",
    "        filename = f\"face_{timestamp}.png\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        speak(\"얼굴 사진을 찍었습니다. 파일명은 \" + filename + \"입니다.\")\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    filename = os.path.join(script_dir, 'voice1.mp3')\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "def main():\n",
    "    face_detector = FaceDetector()\n",
    "\n",
    "    def process_frames():\n",
    "        while face_detector.webcam.isOpened():\n",
    "            face_detector.detect_faces()\n",
    "\n",
    "    # Start a separate thread for frame processing\n",
    "    frame_thread = threading.Thread(target=process_frames)\n",
    "    frame_thread.start()\n",
    "\n",
    "    frame_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# y로 입력받아서 사진찍기\n",
    "import os\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import cv2\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.webcam = cv2.VideoCapture(0)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            print(\"Could not open webcam\")\n",
    "            exit()\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def detect_faces(self):\n",
    "        ret, frame = self.webcam.read()\n",
    "\n",
    "        if ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                speak(\"얼굴이 정면을 향하고 있습니다.\")\n",
    "                speak(\"얼굴 사진을 찍으시겠습니까?\")\n",
    "                response = input(\"얼굴 사진을 찍으시겠습니까? (y/n): \").lower()\n",
    "\n",
    "                if response == 'y':\n",
    "                    self.take_picture(frame)\n",
    "                    \n",
    "                \n",
    "\n",
    "            else:\n",
    "                speak(\"얼굴을 감지할 수 없습니다. 얼굴을 정면으로 향하게 해주세요.\")\n",
    "\n",
    "            with self.lock:\n",
    "                for (x, y, w, h) in faces:\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                self.webcam.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "    def take_picture(self, frame):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y년%m월%d일_%H시%M분%S초\")\n",
    "        filename = f\"face_{timestamp}.png\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        speak(\"얼굴 사진을 찍었습니다. 파일명은 \" + filename + \"입니다.\")\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    filename = os.path.join(script_dir, 'voice1.mp3')\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "def main():\n",
    "    face_detector = FaceDetector()\n",
    "\n",
    "    def process_frames():\n",
    "        while face_detector.webcam.isOpened():\n",
    "            face_detector.detect_faces()\n",
    "\n",
    "    # Start a separate thread for frame processingqn\n",
    "    frame_thread = threading.Thread(target=process_frames)\n",
    "    frame_thread.start()\n",
    "\n",
    "    frame_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "import cv2\n",
    "import datetime\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.webcam = cv2.VideoCapture(0)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            print(\"Could not open webcam\")\n",
    "            exit()\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def detect_faces(self):\n",
    "        ret, frame = self.webcam.read()\n",
    "\n",
    "        if ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                speak(\"얼굴이 정면을 향하고 있습니다.\")\n",
    "                speak(\"얼굴 사진을 찍으시겠습니까?\")\n",
    "\n",
    "                response = listen_for_yes()\n",
    "                \n",
    "                if response:\n",
    "                    self.take_picture(frame)\n",
    "                else:\n",
    "                    speak(\"얼굴 사진을 찍지 않습니다. 프로그램을 종료합니다.\")\n",
    "                    self.webcam.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    exit()\n",
    "\n",
    "            else:\n",
    "                speak(\"얼굴을 감지할 수 없습니다. 얼굴을 정면으로 향하게 해주세요.\")\n",
    "\n",
    "            with self.lock:\n",
    "                for (x, y, w, h) in faces:\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                self.webcam.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "    def take_picture(self, frame):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y년%m월%d일_%H시%M분%S초\")\n",
    "        filename = f\"face_{timestamp}.png\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        speak(\"얼굴 사진을 찍었습니다. 파일명은 \" + filename + \"입니다.\")\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    filename = os.path.join(script_dir, 'voice1.mp3')\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "def listen_for_yes():\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        speak(\"마이크에서 'yes'를 말씀해주세요.\")\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        response = recognizer.recognize_google(audio, language=\"ko-KR\").lower()\n",
    "        print(\"음성 인식 결과:\", response)\n",
    "        return \"yes\" in response\n",
    "    except sr.UnknownValueError:\n",
    "        speak(\"음성을 인식하지 못했습니다.\")\n",
    "        return False\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"음성 인식 서비스에 오류가 발생했습니다: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    face_detector = FaceDetector()\n",
    "\n",
    "    def process_frames():\n",
    "        while face_detector.webcam.isOpened():\n",
    "            face_detector.detect_faces()\n",
    "\n",
    "    # Start a separate thread for frame processing\n",
    "    frame_thread = threading.Thread(target=process_frames)\n",
    "    frame_thread.start()\n",
    "    frame_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import cv2\n",
    "import datetime\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.webcam = cv2.VideoCapture(0)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            print(\"Could not open webcam\")\n",
    "            exit()\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def detect_faces(self):\n",
    "        ret, frame = self.webcam.read()\n",
    "\n",
    "        if ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                speak(\"얼굴이 정면을 향하고 있습니다.\")\n",
    "                speak(\"얼굴 사진을 찍으시겠습니까?\")\n",
    "\n",
    "                response = listen_for_yes()\n",
    "                \n",
    "                if response:\n",
    "                    self.take_picture(frame)\n",
    "                else:\n",
    "                    speak(\"얼굴 사진을 찍지 않습니다. 프로그램을 종료합니다.\")\n",
    "                    self.webcam.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    exit()\n",
    "\n",
    "            else:\n",
    "                speak(\"얼굴을 감지할 수 없습니다. 얼굴을 정면으로 향하게 해주세요.\")\n",
    "\n",
    "            with self.lock:\n",
    "                for (x, y, w, h) in faces:\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                self.webcam.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "    def take_picture(self, frame):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y년%m월%d일_%H시%M분%S초\")\n",
    "        filename = f\"face_{timestamp}.png\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        speak(\"얼굴 사진을 찍었습니다. 파일명은 \" + filename + \"입니다.\")\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    filename = os.path.join(script_dir, 'voice1.mp3')\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "def listen_for_yes():\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        speak(\"마이크에서 'yes'를 말씀해주세요.\")\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        response = recognizer.recognize_google(audio, language=\"ko-KR\").lower()\n",
    "        print(\"음성 인식 결과:\", response)\n",
    "        return \"yes\" in response\n",
    "    except sr.UnknownValueError:\n",
    "        speak(\"음성을 인식하지 못했습니다.\")\n",
    "        return False\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"음성 인식 서비스에 오류가 발생했습니다: {e}\")\n",
    "        return False\n",
    "    \n",
    "def main():\n",
    "    face_detector = FaceDetector()\n",
    "    stop_thread = threading.Event()\n",
    "\n",
    "    def process_frames():\n",
    "        while not stop_thread.is_set():\n",
    "            face_detector.detect_faces()\n",
    "\n",
    "    # 프레임 처리를 위한 별도의 스레드 시작\n",
    "    frame_thread = threading.Thread(target=process_frames)\n",
    "    frame_thread.start()\n",
    "\n",
    "    try:\n",
    "        frame_thread.join()  # 스레드가 자연스럽게 완료될 때까지 기다립니다 (이 경우에는 결코 완료되지 않음)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"인터럽트! 스레드를 중지합니다...\")\n",
    "        stop_thread.set()\n",
    "        frame_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
